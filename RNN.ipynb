{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction-rnn",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) for Sequence Analysis with Keras and TensorFlow\n",
    "\n",
    "This notebook demonstrates how to build and train Recurrent Neural Networks (RNNs) for sequence analysis tasks using Keras and TensorFlow. We will focus on using Long Short-Term Memory (LSTM) networks for sentiment analysis on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RNNs\n",
    "\n",
    "### What are RNNs?\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to work with sequential data. Unlike feedforward neural networks, RNNs have internal memory, allowing them to process sequences of inputs of arbitrary length.  This is achieved through recurrent connections, where the output of a neuron at time *t* is fed back as input to the same neuron at time *t+1*. This feedback loop allows the network to retain information about past inputs, making them suitable for tasks where context is important, such as natural language processing, time series analysis, and speech recognition.\n",
    "\n",
    "**Key features of RNNs:**\n",
    "\n",
    "- **Sequential Processing:** RNNs process input sequences one element at a time, maintaining a hidden state that is updated at each step based on the current input and the previous hidden state.\n",
    "- **Memory:** The hidden state acts as a memory, allowing the network to retain information about past inputs and use it to influence the processing of future inputs.\n",
    "- **Handling Variable Length Sequences:** RNNs can naturally handle input sequences of varying lengths, which is crucial for many real-world applications.\n",
    "\n",
    "### Applications of RNNs\n",
    "RNNs are used in a wide range of applications, including:\n",
    "\n",
    "- **Natural Language Processing (NLP):** Sentiment analysis, machine translation, text generation, language modeling.\n",
    "- **Time Series Analysis:** Stock price prediction, weather forecasting, anomaly detection.\n",
    "- **Speech Recognition:** Converting spoken language into text.\n",
    "- **Video Analysis:** Action recognition, video captioning.\n",
    "\n",
    "### Why RNNs for Sequential Data?\n",
    "RNNs are particularly useful for sequential data because they can capture dependencies between elements in a sequence.  For example, in a sentence, the meaning of a word often depends on the words that came before it.  RNNs can model these dependencies, whereas traditional feedforward networks treat each input independently.\n",
    "\n",
    "### LSTM Architecture\n",
    "In this notebook, we will implement a type of RNN called Long Short-Term Memory (LSTM) network. LSTMs are designed to address the vanishing gradient problem in traditional RNNs, which makes it difficult to learn long-range dependencies in sequences. LSTMs achieve this through a more complex cell structure that includes:\n",
    "\n",
    "- **Cell State:**  Acts as a long-term memory, carrying information across many time steps.\n",
    "- **Hidden State:**  Acts as a short-term memory, similar to the hidden state in a traditional RNN.\n",
    "- **Gates:**  Control the flow of information into and out of the cell state. These include the input gate, forget gate, and output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset: IMDB Reviews for Sentiment Analysis\n",
    "\n",
    "We will use the IMDB dataset for sentiment analysis. This dataset consists of movie reviews from the Internet Movie Database (IMDB), labeled as either positive or negative. It is a widely used dataset for binary sentiment classification and is readily available through Keras datasets. The task is to perform sentiment analysis: we want our model to predict whether a given review is positive or negative.\n",
    "\n",
    "### Key Features\n",
    "- 25,000 training reviews\n",
    "- 25,000 test reviews\n",
    "- Binary sentiment classification (positive/negative)\n",
    "- Average review length: 200-300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For evaluation: classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "vocab_size = 5000 # Consider only the top 5,000 most frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "RNNs require input data to be in a specific format. For text data, this typically involves tokenization, padding, and splitting into training, validation, and test sets.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Padding Sequences:**  Movie reviews have varying lengths. We need to pad sequences to a fixed length so that they can be processed in batches. We will pad sequences to a maximum length of 256 words.\n",
    "- **Splitting into Training and Validation Sets:** We will split the original training set into training and validation sets to monitor the model's performance during training and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences so that each review is of the same length.\n",
    "max_length = 256    # We will fix the maximum review length at 256 words\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_length, padding='pre', truncating='post')\n",
    "x_test_padded = pad_sequences(x_test, maxlen=max_length, padding='pre', truncating='post')\n",
    "\n",
    "print(\"Training data shape after padding:\", x_train_padded.shape)\n",
    "print(\"Test data shape after padding:\", x_test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final, x_val, y_train_final, y_val = train_test_split(x_train_padded, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Final training set shape:\", x_train_final.shape, len(y_train_final))\n",
    "print(\"Validation set shape:\", x_val.shape, len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building\n",
    "\n",
    "We will build an LSTM model for sentiment classification. The model architecture will include:\n",
    "\n",
    "- **Embedding Layer:** Converts word indices into dense vector representations. This layer learns word embeddings during training.\n",
    "- **Bidirectional LSTM Layer:** The core RNN layer that processes the sequences in both forward and backward directions, learning temporal dependencies.\n",
    "- **Dropout Layer:** Regularization to prevent overfitting.\n",
    "- **Dense Layer:** A fully connected layer with a sigmoid activation function for binary classification (positive or negative sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Choices and Rationale\n",
    "\n",
    "- **Embedding Dimension (embedding_dim=256):** A dimension of 256 is a common starting point for word embeddings, providing a balance between expressiveness and computational efficiency. Higher dimensions can capture more semantic information but may lead to overfitting or increased training time.\n",
    "\n",
    "  - **LSTM Units (64):** Sufficient to capture temporal dependencies while keeping the model efficient.\n",
    "  - **Dropout Rates (0.3):** Dropout is used to prevent overfitting. Used after the LSTM layers provides strong regularization before the dense layers.\n",
    "  - **Max Sequence Length (max_length=256):** This value covers the majority of review lengths in the IMDB dataset, ensuring most information is retained while keeping computation manageable.\n",
    "  - **Vocabulary Size (vocab_size=5000):** Limiting to the top 5,000 most frequent words reduces noise from rare words and keeps the embedding matrix size reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Build and return a Sequential LSTM-based RNN model for binary sentiment classification.\n",
    "    \n",
    "    Args:\n",
    "    vocab_size (int): Size of the vocabulary (number of unique words to consider).\n",
    "        embedding_dim (int): Dimension of the embedding vectors.\n",
    "        max_length (int): Maximum length of input sequences\n",
    "    Returns:\n",
    "        keras.Sequential: Compiled RNN model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Embedding layer to learn word embeddings\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "    # LSTM layer with a Bidirectional wrapper\n",
    "    model.add(Bidirectional(LSTM(units=64, dropout=0.2)))\n",
    "    # Dropout layer to reduce overfitting\n",
    "    model.add(Dropout(0.3))\n",
    "    # Dense Layer\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    # Final output layer for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create an RNN model\n",
    "embedding_dim = 256\n",
    "model = create_rnn_model(vocab_size, embedding_dim, max_length)\n",
    "# Explicitly build the model to finalize shapes\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We compile the model with an appropriate loss function, optimizer, and metrics, and then train it.\n",
    "\n",
    "**Compilation:**\n",
    "\n",
    "*   **Loss Function:** `binary_crossentropy` is used because this is a binary classification problem (positive or negative sentiment).\n",
    "*   **Optimizer:** `Adam` optimizer with a learning rate of 0.0005.\n",
    "*   **Metrics:** We track `accuracy` to measure the model's performance.\n",
    "\n",
    "**Training:**\n",
    "\n",
    "*   We train the model using `model.fit()`, providing the training and validation data.\n",
    "*   `epochs` specifies the number of training iterations over the entire dataset.\n",
    "*   `batch_size` defines the number of samples processed in each gradient update.\n",
    "*   We use `EarlyStopping` and `ModelCheckpoint` callbacks for efficient training. `EarlyStopping` prevents overfitting by stopping training when the validation loss stops improving. `ModelCheckpoint` saves the best model based on validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_rnn_model.h5', monitor='val_accuracy', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_final, y_train_final,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stop, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training and Validation Metrics\n",
    "**Accuracy:**\n",
    "The training and validation accuracy curves illustrate how the model's performance improves over epochs. Specifically, it is observed that the training and validation accuracy and loss curves show strong learning and generalization, with minimal overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss:**\n",
    "The training and validation loss curves indicate how well the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we assess the performance of our trained LSTM model on unseen test data. \n",
    "\n",
    "- The first code cell loads the best saved model and computes its loss and accuracy on the padded test set. \n",
    "- Next, predictions are generated using the best saved model, and these output probabilities are thresholded to obtain binary classifications.  \n",
    "- The subsequent code cell prints a detailed classification report—including precision, recall, and F1-score—offering insights into the model's performance on each class. \n",
    "- Finally, a confusion matrix is plotted as a heatmap to clearly visualize the model’s true positives, false negatives, and overall prediction accuracy.\n",
    "\n",
    "The trained model achieves the following performance on the IMDB test set:\n",
    "\n",
    "- **Training Accuracy:** ~0.93\n",
    "- **Validation Accuracy:** ~0.86\n",
    "- **Test Accuracy:** ~0.86\n",
    "- **Test Loss:** ~0.35\n",
    "- **Precision/Recall/F1:** ~0.86 (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_rnn_model.h5')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = best_model.predict(x_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction\n",
    "In this section, we select a random review from the test set and use our best saved model to predict its sentiment. A prediction probability above 0.5 indicates a positive review, while a probability at or below 0.5 indicates a negative review.\n",
    "The code cell below randomly chooses a review from the padded test data, makes a prediction, and prints the review text,predicted sentiment and the associated probability.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get the word index from Keras and build a reverse mapping\n",
    "def decode_review(encoded_review, word_index):\n",
    "    # The first indices are reserved\n",
    "    index_word = {v + 3: k for k, v in word_index.items()}\n",
    "    index_word[0] = \"<PAD>\"\n",
    "    index_word[1] = \"<START>\"\n",
    "    index_word[2] = \"<UNK>\"\n",
    "    index_word[3] = \"<UNUSED>\"\n",
    "    return ' '.join([index_word.get(i, '?') for i in encoded_review])\n",
    "\n",
    "# Load the word index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Select a random index from the padded test set (x_test_padded is available from preprocessing)\n",
    "random_index = np.random.randint(0, len(x_test_padded))\n",
    "print(\"Random review index:\", random_index)\n",
    "\n",
    "# Retrieve the corresponding padded review (ensuring shape is (1, max_length) for prediction)\n",
    "random_review = x_test_padded[random_index:random_index+1]\n",
    "\n",
    "# Use the previously loaded best model to predict the sentiment of the selected review\n",
    "prediction_probability = best_model.predict(random_review)[0][0]\n",
    "predicted_sentiment = \"Positive\" if prediction_probability > 0.5 else \"Negative\"\n",
    "\n",
    "# Decode the original (unpadded) review to text\n",
    "original_review = x_test[random_index]\n",
    "decoded_review = decode_review(original_review, word_index)\n",
    "\n",
    "# Display the review text, prediction probability, and the resulting sentiment classification\n",
    "print(\"Review text:\")\n",
    "print(decoded_review)\n",
    "print(\"\\nPrediction probability: {:.4f}\".format(prediction_probability))\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we successfully built and trained an Bidirectional LSTM network for sentiment analysis using the IMDB dataset. The model achieved a reasonable accuracy on the test set, demonstrating the effectiveness of RNNs for sequence classification tasks and the importance of careful model design and training. We covered data loading, preprocessing, model building, training, evaluation and prediction.\n",
    "\n",
    "**Potential Improvements and Future Work:**\n",
    "\n",
    "*   **Hyperparameter Tuning:** Experiment with different values for `num_words`, `maxlen`, `embedding_dim`, `lstm_units`, and `dropout_rate` to optimize model performance.  Techniques like grid search or random search can be used.\n",
    "*   **Different RNN Architectures:** Explore other RNN architectures, such as GRUs (Gated Recurrent Units), which are often faster to train than LSTMs and can achieve comparable performance.\n",
    "*   **Pre-trained Word Embeddings:** Utilize pre-trained word embeddings like Word2Vec or GloVe instead of learning embeddings from scratch.  This can improve performance, especially with limited training data.\n",
    "*   **Attention Mechanisms:** Incorporate attention mechanisms to allow the model to focus on the most relevant parts of the input sequence.\n",
    "*   **Deeper Models:** Experiment with stacking multiple LSTM layers to create a deeper model, although this can increase training time and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "\n",
    "*   **Keras:** [https://keras.io/](https://keras.io/)\n",
    "*   **TensorFlow:** [https://www.tensorflow.org/](https://www.tensorflow.org/)\n",
    "*   **IMDB Dataset:** [https://keras.io/api/datasets/imdb/](https://keras.io/api/datasets/imdb/)\n",
    "*   **scikit-learn:** [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
